{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to apply the slurm restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_THREADS = int(os.environ['OMP_NUM_THREADS'])\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=NUM_THREADS,\n",
    "    inter_op_parallelism_threads=NUM_THREADS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenient functions to show the running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "number_inputs = 66\n",
    "number_outputs = 8\n",
    "seq_len = 400 # max 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent loading data: 0m 41s\n",
      "X_train: (64, 400, 66)\n",
      "X_valid: (1267, 683, 66)\n",
      "X_test: (1267, 687, 66)\n"
     ]
    }
   ],
   "source": [
    "# Get all the data\n",
    "trainList_addr = './data/trainList'\n",
    "validList_addr = './data/validList'\n",
    "testList_addr = './data/testList'\n",
    "\n",
    "start = time.time()\n",
    "train_list, train_len_list = read_list(trainList_addr)\n",
    "valid_list, valid_len_list = read_list(validList_addr)\n",
    "test_list, test_len_list = read_list(testList_addr)\n",
    "\n",
    "train_generator = generate_batch(train_list, train_len_list,\n",
    "                                 max_seq_length=seq_len,\n",
    "                                 batch_size=batch_size)\n",
    "\n",
    "X_train, t_train, len_train, mask_train = train_generator.next()\n",
    "# X_train, t_train, len_train = read_data(train_list, train_len_list,\n",
    "#                                         max_seq_length=seq_len)\n",
    "X_valid, t_valid, len_valid, mask_valid = read_data(valid_list, valid_len_list,\n",
    "                                        max_seq_length=max(valid_len_list))\n",
    "X_test, t_test, len_test, mask_test = read_data(test_list, test_len_list,\n",
    "                                     max_seq_length=max(test_len_list))\n",
    "\n",
    "timeSpent = time.time() - start\n",
    "print(\"Time spent loading data: {}\".format(asMinutes(timeSpent)))\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267, 687)\n",
      "(1267, 687)\n"
     ]
    }
   ],
   "source": [
    "print(t_test.shape)\n",
    "print(mask_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_layers = 3\n",
    "state_size = 100\n",
    "num_units_l1 = 100\n",
    "dropout = False\n",
    "dropout_keep_rate = 0.5\n",
    "clip_gradients = True\n",
    "max_grad_norm = 5\n",
    "\n",
    "reset_default_graph()\n",
    "\n",
    "X_input = tf.placeholder(tf.float32, shape=[None, None, number_inputs], name='X_input')\n",
    "X_length = tf.placeholder(tf.int32, shape=[None,], name='X_length')\n",
    "t_input = tf.placeholder(tf.int32, shape=[None, None], name='t_input')\n",
    "X_mask = tf.placeholder(tf.int32, shape=[None, None], name='X_mask')\n",
    "\n",
    "def GRU_with_dropout(dropout=True):\n",
    "    if dropout:\n",
    "        return rnn.DropoutWrapper(rnn.GRUCell(state_size),\n",
    "                                  output_keep_prob=dropout_keep_rate)\n",
    "    else:\n",
    "        return rnn.GRUCell(state_size)\n",
    "\n",
    "cells = rnn.MultiRNNCell([GRU_with_dropout(dropout=dropout) for _ in range(num_layers)],\n",
    "                         state_is_tuple=True)\n",
    "# print(cells.state_size)\n",
    "# init_state = (tf.zeros([batch_size, state_size]), ) * 3\n",
    "rnn_outputs, output_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cells, cell_bw=cells, inputs=X_input,\n",
    "                                                 sequence_length=X_length, \n",
    "#                                                  initial_state_fw=init_state, initial_state_bw=init_state,\n",
    "                                                 dtype=tf.float32)\n",
    "# print(output_state[0][0].shape)\n",
    "enc_outputs = tf.concat(rnn_outputs, 2)\n",
    "outputs = tf.reshape(enc_outputs, [-1, state_size*2])\n",
    "l1 = fully_connected(outputs, num_units_l1, normalizer_fn=batch_norm)\n",
    "l_out = fully_connected(l1, number_outputs, activation_fn=None)\n",
    "\n",
    "batch_size_shp = tf.shape(enc_outputs)[0]\n",
    "seq_len_shp = tf.shape(enc_outputs)[1]\n",
    "l_out_reshape = tf.reshape(l_out, [batch_size_shp, seq_len_shp, number_outputs])\n",
    "\n",
    "y = l_out_reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code use softmax_cross_entropy_with_logits to resolve the NaN problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This piece is working!!!**\n",
    "\n",
    "The following code uses sparse_softmax_cross_entropy_with_logits but with number_outputs = 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the mask to filter out zero-padding when calculating loss and accuracy\n",
    "X_mask = tf.to_float(X_mask)\n",
    "mask_sum = tf.reduce_sum(X_mask)\n",
    "# Calculate loss\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=t_input, logits=y)\n",
    "# print(cross_entropy.dtype)\n",
    "cross_entropy *= X_mask\n",
    "# tf.reduce_mean cannot be used because it will count zero-padding for weights.\n",
    "loss = tf.reduce_sum(cross_entropy) / mask_sum\n",
    "# Calculate accuracy. Need to exclude the padded zeros.\n",
    "\n",
    "predictions = tf.to_int32(tf.argmax(y, 2))\n",
    "correct = tf.to_float(tf.equal(predictions, t_input))\n",
    "total_correct_preds = tf.reduce_sum(correct * X_mask)\n",
    "accuracy =  total_correct_preds / mask_sum\n",
    "\n",
    "# use global step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# pick optimizer, try momentum or adadelta\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "grads = [element[0] for element in grads_and_vars]\n",
    "variables =  [element[1] for element in grads_and_vars]\n",
    "if clip_gradients:\n",
    "    grads = tf.clip_by_global_norm(grads, max_grad_norm)[0]\n",
    "    \n",
    "grad_norm = tf.global_norm(grads)\n",
    "grads_and_vars = [(grads[i], variables[i]) for i in range(len(grads))]\n",
    "# apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\" * 10 + \"validating the model\"+ \"=\" * 10)\n",
    "# # test validation part\n",
    "# # sess.run(tf.global_variables_initializer())\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     feed_dict = {X_input: X_valid, X_length: len_valid,\n",
    "#                  t_input: t_valid, X_mask: mask_valid}\n",
    "#     fetches = [y, t_input, predictions, cross_entropy, enc_outputs, rnn_outputs]\n",
    "#     res = tuple(sess.run(fetches=fetches, feed_dict=feed_dict))\n",
    "#     print(\"y:\", res[0].shape)\n",
    "#     print(\"t_input\", res[1].shape)\n",
    "#     print(\"predictions\", res[2].shape)\n",
    "#     print(\"Cross Entropy\", res[3].shape)\n",
    "#     print(\"enc_outputs\", res[4].shape)\n",
    "#     print(\"rnn_outputs\", res[5][0].shape, res[5][1].shape)\n",
    "#     # print(len_valid[0])\n",
    "#     # print(res[5][0][0, 144:147, 0:10])\n",
    "# print(\"=\" * 10 + \"Model validation finished\"+ \"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, len_data, mask_data):\n",
    "    num_examples = X_data.shape[0]\n",
    "    sess = tf.get_default_session()\n",
    "    total_correct = 0\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x = X_data[offset:offset+batch_size, :, :]\n",
    "        batch_y = y_data[offset:offset+batch_size, :]\n",
    "        batch_len = len_data[offset:offset+batch_size]\n",
    "        batch_mask = mask_data[offset:offset+batch_size, :]\n",
    "        \n",
    "        feed = {X_input: batch_x, X_length: batch_len,\n",
    "                t_input: batch_y, X_mask: batch_mask}\n",
    "        \n",
    "        res = sess.run(total_correct_preds, feed_dict=feed_dict_val)\n",
    "        total_correct += res\n",
    "    return total_correct / np.sum(mask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_processed: 640, batch_cost: 1.433, validation_accs: 0.5035, patience_count: 0\n",
      "samples_processed: 1280, batch_cost: 1.163, validation_accs: 0.6197, patience_count: 0\n",
      "samples_processed: 1920, batch_cost: 0.982, validation_accs: 0.6684, patience_count: 0\n",
      "samples_processed: 2560, batch_cost: 0.910, validation_accs: 0.6876, patience_count: 0\n",
      "samples_processed: 3200, batch_cost: 0.873, validation_accs: 0.7014, patience_count: 0\n",
      "samples_processed: 3840, batch_cost: 0.845, validation_accs: 0.7052, patience_count: 0\n",
      "samples_processed: 4480, batch_cost: 0.830, validation_accs: 0.7079, patience_count: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8a1e1e5e17f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0msaver1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./bi_gru_300_dp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "val_interval = batch_size * 5\n",
    "print_interval = batch_size * 10\n",
    "samples_to_process = 5e3\n",
    "early_stopping = True\n",
    "patience = 4\n",
    "patience_count = 0\n",
    "\n",
    "samples_processed = 0\n",
    "samples_val = []\n",
    "costs, accs_val, grads_norm = [], [], []\n",
    "saver1 = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "while samples_processed < samples_to_process:\n",
    "    fetches_tr = [train_op, loss, accuracy, grad_norm]\n",
    "    feed_dict_tr = {X_input: X_train, X_length: len_train,\n",
    "                    t_input: t_train, X_mask: mask_train}\n",
    "    res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "    _, batch_cost, batch_acc, batch_grad_norm = res\n",
    "    samples_processed += batch_size\n",
    "    grads_norm += [batch_grad_norm]\n",
    "\n",
    "    #validation data\n",
    "    if samples_processed % val_interval == 0:\n",
    "        acc_val = evaluate(X_valid, t_valid, len_valid, mask_valid)\n",
    "        costs += [batch_cost]\n",
    "        samples_val += [samples_processed]\n",
    "        accs_val += [acc_val]\n",
    "\n",
    "        if samples_processed % print_interval == 0:\n",
    "            print(\"samples_processed: %d, batch_cost: %.3f, validation_accs: %.4f, patience_count: %d\" % \\\n",
    "                  (samples_processed, batch_cost, acc_val, patience_count))\n",
    "\n",
    "        if early_stopping:\n",
    "            if len(accs_val) > patience and acc_val < accs_val[-2]:\n",
    "                patience_count += 1\n",
    "            if patience_count >= patience:\n",
    "                break\n",
    "\n",
    "saver1.save(sess, './bi_gru_300_dp')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetches_test = [accuracy, loss]\n",
    "# feed_dict_test = {X_input: X_test, X_length: len_test,\n",
    "#                   t_input: t_test, X_mask: mask_test}\n",
    "# res_test = tuple(sess.run(fetches=fetches_test, feed_dict=feed_dict_test))\n",
    "\n",
    "# acc_test, loss_test = res_test\n",
    "# print(\"Test Accuracy: {:.4f}\".format(acc_test))\n",
    "# # print(type(gradients_and_vars))\n",
    "# # print(predictions[0, :])\n",
    "# # print(t_test[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"feat66_len300_dynamic_padding.txt\",\"w\") as f:\n",
    "#     for (sample_val, acc_val, cost) in zip(samples_val, accs_val, costs):\n",
    "#         f.write(\"{0},{1},{2}\\n\".format(sample_val, acc_val, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(samples_val, accs_val, 'b-')\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=15)\n",
    "ax1.set_xlabel('Processed samples', fontsize=15)\n",
    "plt.title('Accuracy & Cost with seq_len=300, dynamic padding', fontsize=20)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(samples_val, costs, 'r-')\n",
    "ax2.set_ylabel('Training Cost', fontsize=15)\n",
    "plt.grid('on')\n",
    "plt.savefig(\"out.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(grads_norm))\n",
    "plt.plot(np.arange(len(grads_norm)), grads_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to plot the stored information for old experiment and is not being used currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples_30, accs_30, costs_30 = [], [], []\n",
    "# with open(\"feat66_len30.txt\",\"r\") as f:\n",
    "#     for line in f:\n",
    "#         line = line.split(',')\n",
    "#         samples_30.append(line[0])\n",
    "#         accs_30.append(line[1])\n",
    "#         costs_30.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots()\n",
    "# plt.plot(samples_30, accs_30, 'b-')\n",
    "# ax1.set_ylabel('Validation Accuracy', fontsize=15)\n",
    "# ax1.set_xlabel('Processed samples', fontsize=15)\n",
    "# plt.title('Accuracy & Cost with seq_len=30', fontsize=20)\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(samples_30, costs_30, 'r-')\n",
    "# ax2.set_ylabel('Training Cost', fontsize=15)\n",
    "# plt.grid('on')\n",
    "# plt.savefig(\"out.png\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.3]",
   "language": "python",
   "name": "conda-env-tf-1.3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
