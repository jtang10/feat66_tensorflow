{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "number_inputs = 66\n",
    "number_outputs = 8\n",
    "max_seq_len = 400 # max 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent 17.2s to load the data\n",
      "X_train: (64, 400, 66)\n",
      "X_valid: (1267, 683, 66)\n"
     ]
    }
   ],
   "source": [
    "# Get all the data\n",
    "relative_path = './data/SetOf7604Proteins/'\n",
    "trainList_addr = relative_path + 'trainList'\n",
    "validList_addr = relative_path + 'validList'\n",
    "testList_addr = relative_path + 'testList'\n",
    "\n",
    "start = time.time()\n",
    "train_list = read_list(trainList_addr)\n",
    "valid_list = read_list(validList_addr)\n",
    "test_list = read_list(testList_addr)\n",
    "\n",
    "train_generator = generate_batch(train_list, relative_path, max_seq_len, batch_size)\n",
    "\n",
    "X_train, t_train, len_train, mask_train = train_generator.next()\n",
    "X_valid, t_valid, len_valid, mask_valid = read_data(valid_list, relative_path, max_seq_length=683) # 683\n",
    "\n",
    "timeSpent = time.time() - start\n",
    "print(\"Spent {:.1f}s to load the data\".format(timeSpent))\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 400, 66)\n",
      "(64, 400)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(mask_train.shape)\n",
    "print(len_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_layers = 2\n",
    "state_size = 100\n",
    "num_units_l1 = 100\n",
    "dropout = False\n",
    "dropout_keep_rate = 0.5\n",
    "clip_gradients = False\n",
    "max_grad_norm = 5\n",
    "attention_size = 50\n",
    "\n",
    "reset_default_graph()\n",
    "\n",
    "X_input = tf.placeholder(tf.float32, shape=[None, None, number_inputs], name='X_input')\n",
    "X_length = tf.placeholder(tf.int32, shape=[None,], name='X_length')\n",
    "t_input = tf.placeholder(tf.int32, shape=[None, None], name='t_input')\n",
    "X_mask = tf.placeholder(tf.int32, shape=[None, None], name='X_mask')\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "def GRU_with_dropout(dropout=True):\n",
    "    if dropout:\n",
    "        return rnn.DropoutWrapper(rnn.GRUCell(state_size),\n",
    "                                  output_keep_prob=dropout_keep_rate)\n",
    "    else:\n",
    "        return rnn.GRUCell(state_size)\n",
    "\n",
    "cells = rnn.MultiRNNCell([GRU_with_dropout(dropout=dropout) for _ in range(num_layers)],\n",
    "                         state_is_tuple=True)\n",
    "outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cells, cell_bw=cells, inputs=X_input,\n",
    "                                             sequence_length=X_length, \n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "outputs = tf.concat(outputs, 2)\n",
    "batch_size_shp = tf.shape(outputs)[0]\n",
    "seq_len_shp = tf.shape(outputs)[1]\n",
    "outputs = tf.reshape(outputs, [-1, state_size*2])\n",
    "# bn1 = batch_norm(outputs, center=True, scale=True, is_training=phase)\n",
    "l1 = fully_connected(outputs, num_units_l1) # , normalizer_fn=batch_norm\n",
    "# bn2 = batch_norm(l1, center=True, scale=True, is_training=phase)\n",
    "l_out = fully_connected(l1, number_outputs, activation_fn=None)\n",
    "y = tf.reshape(l_out, [batch_size_shp, seq_len_shp, number_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This piece is working!!!**\n",
    "\n",
    "The following code uses sparse_softmax_cross_entropy_with_logits but with number_outputs = 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mask = tf.to_float(X_mask)\n",
    "mask_sum = tf.reduce_sum(X_mask)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=t_input, logits=y)\n",
    "cross_entropy *= X_mask\n",
    "loss = tf.reduce_sum(cross_entropy) / mask_sum\n",
    "\n",
    "predictions = tf.to_int32(tf.argmax(y, 2))\n",
    "correct = tf.to_float(tf.equal(predictions, t_input))\n",
    "total_correct_preds = tf.reduce_sum(correct * X_mask)\n",
    "accuracy =  total_correct_preds / mask_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "grads = [element[0] for element in grads_and_vars]\n",
    "variables =  [element[1] for element in grads_and_vars]\n",
    "if clip_gradients:\n",
    "    grads = tf.clip_by_global_norm(grads, max_grad_norm)[0]   \n",
    "grad_norm = tf.global_norm(grads)\n",
    "grads_and_vars = [(grads[i], variables[i]) for i in range(len(grads))]\n",
    "\n",
    "# update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "# with tf.control_dependencies(update_ops):\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\" * 10 + \"validating the model\"+ \"=\" * 10)\n",
    "# # test validation part\n",
    "# # sess.run(tf.global_variables_initializer())\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     feed_dict = {X_input: X_valid, X_length: len_valid,\n",
    "#                  t_input: t_valid, X_mask: mask_valid}\n",
    "#     fetches = [enc_outputs, outputs, l_out, l_out_reshape, y]\n",
    "#     res = tuple(sess.run(fetches=fetches, feed_dict=feed_dict))\n",
    "#     for element in res:\n",
    "#         print(element.shape)\n",
    "# print(\"=\" * 10 + \"Model validation finished\"+ \"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, len_data, mask_data):\n",
    "    num_examples = X_data.shape[0]\n",
    "    sess = tf.get_default_session()\n",
    "    total_correct = 0\n",
    "    loss_total = 0\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x = X_data[offset:offset+batch_size, :, :]\n",
    "        batch_y = y_data[offset:offset+batch_size, :]\n",
    "        batch_len = len_data[offset:offset+batch_size]\n",
    "        batch_mask = mask_data[offset:offset+batch_size, :]\n",
    "        \n",
    "        feed = {X_input: batch_x, X_length: batch_len,\n",
    "                t_input: batch_y, X_mask: batch_mask,\n",
    "                phase: False}\n",
    "        \n",
    "        loss_val, correct_val = sess.run([loss, total_correct_preds], feed_dict=feed)\n",
    "        total_correct += correct_val\n",
    "        loss_total += loss_val\n",
    "    return total_correct / np.sum(mask_data), loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_individual(protein_name, CASP):\n",
    "    features, labels, seq_len, mask = read_protein(protein_name, relative_path, expand_dims=True, CASP=CASP)\n",
    "    feed = {X_input: features, X_length: seq_len,\n",
    "            t_input: labels, X_mask: mask, phase: False}\n",
    "    sess = tf.get_default_session()\n",
    "    loss_val, correct_val = sess.run([loss, total_correct_preds], feed_dict=feed)\n",
    "    return correct_val, np.sum(mask), correct_val / np.sum(mask), loss_val\n",
    "\n",
    "def sequential_evaluate(dataList, CASP=False, print_interval=100):\n",
    "    loss_total = 0\n",
    "    total_correct = 0\n",
    "    mask_total = 0\n",
    "    for i, protein_name in enumerate(dataList):\n",
    "        correct_val, mask_sum, _, loss_val = evaluate_individual(protein_name, CASP)\n",
    "        total_correct += correct_val\n",
    "        loss_total += loss_val\n",
    "        mask_total += mask_sum\n",
    "        if i % print_interval == 0:\n",
    "            print(\"Number of processed proteins\", i)\n",
    "            print(\"Snapshot: Accuracy: {}; loss: {}\".format(total_correct / mask_total, loss_total))\n",
    "    return total_correct / mask_total, loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid_accuracy 37.5% is best so far. Saving model...\n",
      "Current valid_accuracy 45.1% is best so far. Saving model...\n",
      "samples_processed: 640, batch_cost: 1.481, validation_accs: 0.4508, validation_loss: 29.9877\n",
      "Current valid_accuracy 48.5% is best so far. Saving model...\n",
      "Current valid_accuracy 54.6% is best so far. Saving model...\n",
      "samples_processed: 1280, batch_cost: 1.297, validation_accs: 0.5463, validation_loss: 26.3593\n",
      "Current valid_accuracy 57.0% is best so far. Saving model...\n",
      "Current valid_accuracy 60.8% is best so far. Saving model...\n",
      "samples_processed: 1920, batch_cost: 1.080, validation_accs: 0.6076, validation_loss: 22.1347\n",
      "Current valid_accuracy 64.2% is best so far. Saving model...\n",
      "Current valid_accuracy 65.3% is best so far. Saving model...\n",
      "samples_processed: 2560, batch_cost: 0.916, validation_accs: 0.6531, validation_loss: 19.3161\n",
      "Current valid_accuracy 66.5% is best so far. Saving model...\n",
      "Current valid_accuracy 67.2% is best so far. Saving model...\n",
      "samples_processed: 3200, batch_cost: 0.855, validation_accs: 0.6717, validation_loss: 18.5032\n",
      "Current valid_accuracy 67.8% is best so far. Saving model...\n",
      "Current valid_accuracy 68.1% is best so far. Saving model...\n",
      "samples_processed: 3840, batch_cost: 0.825, validation_accs: 0.6814, validation_loss: 18.0562\n",
      "Current valid_accuracy 68.3% is best so far. Saving model...\n",
      "Current valid_accuracy 68.6% is best so far. Saving model...\n",
      "samples_processed: 4480, batch_cost: 0.804, validation_accs: 0.6855, validation_loss: 17.7888\n",
      "Current valid_accuracy 68.7% is best so far. Saving model...\n",
      "Current valid_accuracy 68.9% is best so far. Saving model...\n",
      "samples_processed: 5120, batch_cost: 0.787, validation_accs: 0.6889, validation_loss: 17.5911\n",
      "Current valid_accuracy 69.0% is best so far. Saving model...\n",
      "Current valid_accuracy 69.1% is best so far. Saving model...\n",
      "samples_processed: 5760, batch_cost: 0.772, validation_accs: 0.6914, validation_loss: 17.4430\n",
      "Current valid_accuracy 69.2% is best so far. Saving model...\n",
      "samples_processed: 6400, batch_cost: 0.769, validation_accs: 0.6924, validation_loss: 17.3775\n",
      "Current valid_accuracy 69.5% is best so far. Saving model...\n",
      "samples_processed: 7040, batch_cost: 0.751, validation_accs: 0.6941, validation_loss: 17.2724\n",
      "Current valid_accuracy 69.6% is best so far. Saving model...\n",
      "Current valid_accuracy 69.6% is best so far. Saving model...\n",
      "samples_processed: 7680, batch_cost: 0.742, validation_accs: 0.6964, validation_loss: 17.2346\n",
      "Current valid_accuracy 69.7% is best so far. Saving model...\n",
      "samples_processed: 8320, batch_cost: 0.732, validation_accs: 0.6966, validation_loss: 17.2264\n",
      "samples_processed: 8960, batch_cost: 0.723, validation_accs: 0.6963, validation_loss: 17.2257\n",
      "samples_processed: 9600, batch_cost: 0.714, validation_accs: 0.6961, validation_loss: 17.2641\n",
      "time spent: 442.733 seconds\n"
     ]
    }
   ],
   "source": [
    "val_interval = batch_size * 5\n",
    "print_interval = batch_size * 10\n",
    "samples_to_process = 1e4\n",
    "samples_processed = 0\n",
    "samples_val = []\n",
    "costs, accs_val, grads_norm = [], [], []\n",
    "saver1 = tf.train.Saver()\n",
    "\n",
    "NUM_THREADS = int(os.environ['OMP_NUM_THREADS'])\n",
    "config_slurm = tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS, \n",
    "                        inter_op_parallelism_threads=NUM_THREADS)\n",
    "\n",
    "with tf.Session(config=config_slurm) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    start = time.time()\n",
    "    while samples_processed < samples_to_process:\n",
    "        fetches_tr = [train_op, loss, accuracy, grad_norm]\n",
    "        feed_dict_tr = {X_input: X_train, X_length: len_train,\n",
    "                        t_input: t_train, X_mask: mask_train,\n",
    "                        phase: True}\n",
    "        res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "        _, batch_cost, batch_acc, batch_grad_norm = res\n",
    "        samples_processed += batch_size\n",
    "#         print(\"{} samples processed\".format(samples_processed))\n",
    "        grads_norm += [batch_grad_norm]\n",
    "\n",
    "        #validation data\n",
    "        if samples_processed % val_interval == 0:\n",
    "            acc_val, acc_loss = evaluate(X_valid, t_valid, len_valid, mask_valid)\n",
    "            costs += [batch_cost]\n",
    "            samples_val += [samples_processed]\n",
    "            accs_val += [acc_val]\n",
    "            if accs_val[-1] >= max(accs_val):\n",
    "                print(\"Current valid_accuracy {:.1f}% is best so far. Saving model...\".format(accs_val[-1]*100))\n",
    "                saver1.save(sess, './BiGRU_noBN')\n",
    "            if samples_processed % print_interval == 0:\n",
    "                print(\"samples_processed: %d, batch_cost: %.3f, validation_accs: %.4f, validation_loss: %.4f\" % \\\n",
    "                      (samples_processed, batch_cost, acc_val, acc_loss))\n",
    "\n",
    "    print(\"time spent: {:.3f} seconds\".format(time.time() - start))\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./BiGRU_noBN\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key BatchNorm/moving_variance not found in checkpoint\n\t [[Node: save_1/RestoreV2_7 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_7/tensor_names, save_1/RestoreV2_7/shape_and_slices)]]\n\nCaused by op u'save_1/RestoreV2_7', defined at:\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-ddfabb62eb45>\", line 2, in <module>\n    new_saver = tf.train.import_meta_graph('BiGRU_BN.meta')\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\n    **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\n    op_def=op_def)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key BatchNorm/moving_variance not found in checkpoint\n\t [[Node: save_1/RestoreV2_7 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_7/tensor_names, save_1/RestoreV2_7/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ddfabb62eb45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BiGRU_BN.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnew_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1560\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key BatchNorm/moving_variance not found in checkpoint\n\t [[Node: save_1/RestoreV2_7 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_7/tensor_names, save_1/RestoreV2_7/shape_and_slices)]]\n\nCaused by op u'save_1/RestoreV2_7', defined at:\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-ddfabb62eb45>\", line 2, in <module>\n    new_saver = tf.train.import_meta_graph('BiGRU_BN.meta')\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\n    **kwargs)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\n    op_def=op_def)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home-nfs/jtang7/anaconda2/envs/tf-1.3/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key BatchNorm/moving_variance not found in checkpoint\n\t [[Node: save_1/RestoreV2_7 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_7/tensor_names, save_1/RestoreV2_7/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('BiGRU_noBN.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Model restored\")\n",
    "    start = time.time()\n",
    "    acc_valid, loss_valid = sequential_evaluate(valid_list)\n",
    "    print(\"finished in {:.1f} seconds\".format(time.time() - start))\n",
    "    print(\"Accuracy: {}; loss: {}\".format(acc_valid, loss_valid))\n",
    "#     for i in range(10):\n",
    "#         n_correct, acc, loss_ = evaluate_individual(valid_list[i])\n",
    "#         print(n_correct, acc, loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(samples_val, accs_val, 'b-')\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=15)\n",
    "ax1.set_xlabel('Processed samples', fontsize=15)\n",
    "plt.title('Accuracy & Cost with BN', fontsize=20)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(samples_val, costs, 'r-')\n",
    "ax2.set_ylabel('Training Cost', fontsize=15)\n",
    "plt.grid('on')\n",
    "plt.savefig(\"out.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples_noBN = samples_val[:]\n",
    "# accs_val_noBN = accs_val[:]\n",
    "# costs_train_noBN = costs[:]\n",
    "\n",
    "# with open(\"result_noBN.txt\",\"w\") as f:\n",
    "#     for (sample_val, acc_val, cost) in zip(samples_noBN, accs_val_noBN, costs_train_noBN):\n",
    "#         f.write(\"{0},{1},{2}\\n\".format(sample_val, acc_val, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_noBN, accs_val_noBN, costs_train_noBN = [], [], []\n",
    "with open(\"result_noBN.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.split(',')\n",
    "        samples_noBN.append(line[0])\n",
    "        accs_val_noBN.append(line[1])\n",
    "        costs_train_noBN.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(samples_noBN, accs_val_noBN, 'b-')\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=15)\n",
    "ax1.set_xlabel('Processed samples', fontsize=15)\n",
    "plt.title('Accuracy & Cost without BN', fontsize=20)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(samples_noBN, costs_train_noBN, 'r-')\n",
    "ax2.set_ylabel('Training Cost', fontsize=15)\n",
    "plt.grid('on')\n",
    "plt.savefig(\"out.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grads_norm))\n",
    "plt.plot(np.arange(len(grads_norm)), grads_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./BiGRU_noBN\n",
      "Model restored\n",
      "Number of processed proteins 0\n",
      "Snapshot: Accuracy: 0.660550458716; loss: 1.04222917557\n",
      "Number of processed proteins 100\n",
      "Snapshot: Accuracy: 0.673852372245; loss: 90.547955066\n",
      "Number of processed proteins 200\n",
      "Snapshot: Accuracy: 0.683326343834; loss: 171.967855833\n",
      "Number of processed proteins 300\n",
      "Snapshot: Accuracy: 0.696296903162; loss: 249.610781245\n",
      "Number of processed proteins 400\n",
      "Snapshot: Accuracy: 0.698432955787; loss: 334.137141638\n",
      "Number of processed proteins 500\n",
      "Snapshot: Accuracy: 0.698249997541; loss: 417.02331835\n",
      "Number of processed proteins 600\n",
      "Snapshot: Accuracy: 0.699387616988; loss: 494.954165161\n",
      "Number of processed proteins 700\n",
      "Snapshot: Accuracy: 0.701359147081; loss: 570.458231442\n",
      "Number of processed proteins 800\n",
      "Snapshot: Accuracy: 0.700909983318; loss: 651.645652674\n",
      "Number of processed proteins 900\n",
      "Snapshot: Accuracy: 0.699774349286; loss: 735.816624306\n",
      "Number of processed proteins 1000\n",
      "Snapshot: Accuracy: 0.700398939933; loss: 815.845463119\n",
      "Number of processed proteins 1100\n",
      "Snapshot: Accuracy: 0.69986687695; loss: 898.125948705\n",
      "Number of processed proteins 1200\n",
      "Snapshot: Accuracy: 0.698868726513; loss: 983.720050685\n",
      "finished in 234.8 seconds\n",
      "Accuracy: 0.699301654326; loss: 1035.15540909\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    new_saver = tf.train.import_meta_graph('BiGRU_noBN.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Model restored\")\n",
    "    start = time.time()\n",
    "    acc_test, loss_test = sequential_evaluate(test_list)\n",
    "    print(\"finished in {:.1f} seconds\".format(time.time() - start))\n",
    "    print(\"Accuracy: {}; loss: {}\".format(acc_test, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./BiGRU_noBN\n",
      "Model restored\n",
      "Number of processed proteins 0\n",
      "Snapshot: Accuracy: 0.75; loss: 0.713340342045\n",
      "Number of processed proteins 8\n",
      "Snapshot: Accuracy: 0.666666666667; loss: 7.60328298807\n",
      "Number of processed proteins 16\n",
      "Snapshot: Accuracy: 0.662435862814; loss: 15.3636464477\n",
      "Number of processed proteins 24\n",
      "Snapshot: Accuracy: 0.6669015325; loss: 22.9273266792\n",
      "Number of processed proteins 32\n",
      "Snapshot: Accuracy: 0.665755564233; loss: 30.5735407472\n",
      "Number of processed proteins 40\n",
      "Snapshot: Accuracy: 0.667281483759; loss: 38.131865263\n",
      "Number of processed proteins 48\n",
      "Snapshot: Accuracy: 0.676953494369; loss: 44.3409409821\n",
      "Number of processed proteins 56\n",
      "Snapshot: Accuracy: 0.686832226393; loss: 50.5891824961\n",
      "Number of processed proteins 64\n",
      "Snapshot: Accuracy: 0.688131951466; loss: 57.3169320226\n",
      "finished in 17.1 seconds\n",
      "Accuracy: 0.689448855137; loss: 63.448605597\n"
     ]
    }
   ],
   "source": [
    "relative_path = './data/CASP11/'\n",
    "CASP11_addr = relative_path + 'proteinList'\n",
    "CASP11_list = read_list(CASP11_addr)\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('BiGRU_noBN.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Model restored\")\n",
    "    start = time.time()\n",
    "    acc_11, loss_11 = sequential_evaluate(CASP11_list, CASP=True, print_interval=8)\n",
    "    print(\"finished in {:.1f} seconds\".format(time.time() - start))\n",
    "    print(\"Accuracy: {}; loss: {}\".format(acc_11, loss_11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./BiGRU_noBN\n",
      "Model restored\n",
      "Number of processed proteins 0\n",
      "Snapshot: Accuracy: 0.433628318584; loss: 1.61523652077\n",
      "Number of processed proteins 5\n",
      "Snapshot: Accuracy: 0.668016194332; loss: 6.57240086794\n",
      "Number of processed proteins 10\n",
      "Snapshot: Accuracy: 0.685314685315; loss: 10.0399840772\n",
      "Number of processed proteins 15\n",
      "Snapshot: Accuracy: 0.675903614458; loss: 15.0433500111\n",
      "Number of processed proteins 20\n",
      "Snapshot: Accuracy: 0.659784757303; loss: 19.8404586613\n",
      "Number of processed proteins 25\n",
      "Snapshot: Accuracy: 0.654732797765; loss: 26.1281520128\n",
      "Number of processed proteins 30\n",
      "Snapshot: Accuracy: 0.650573553511; loss: 31.528159976\n",
      "Number of processed proteins 35\n",
      "Snapshot: Accuracy: 0.650206207944; loss: 37.0465227365\n",
      "finished in 10.6 seconds\n",
      "Accuracy: 0.651434543036; loss: 41.349455893\n"
     ]
    }
   ],
   "source": [
    "relative_path = './data/CASP12/'\n",
    "CASP12_addr = relative_path + 'proteinList'\n",
    "CASP12_list = read_list(CASP12_addr)\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('BiGRU_noBN.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Model restored\")\n",
    "    start = time.time()\n",
    "    acc_12, loss_12 = sequential_evaluate(CASP12_list, CASP=False, print_interval=5)\n",
    "    print(\"finished in {:.1f} seconds\".format(time.time() - start))\n",
    "    print(\"Accuracy: {}; loss: {}\".format(acc_12, loss_12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.3]",
   "language": "python",
   "name": "conda-env-tf-1.3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
